{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753353cf-0019-44da-a659-3718bc81cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import gzip\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546be31f-8281-4b5d-b416-cfdfc5ee01b0",
   "metadata": {},
   "source": [
    "# Load and preprocess sensor data\n",
    "Because of the huge size of this data, loading it takes many hours. I often had crashes while trying to find the best way to preprocess. To make sure I don't have to start all over again, I frequently saved the current data frame to csv, i.e., I had multiple \"checkpoints\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5c874-8dba-44c9-8dfa-1e9a37f1a85e",
   "metadata": {},
   "source": [
    "## Load all files from NDW/flow/input that end with 05-08\n",
    "This means that they have the data for the times between 5am and 9am.\n",
    "</br> The resulting data frame is used in link_obs_exp.ipynb to get the observed counts. After this step, I inspect the data (mostly a sample of the data) and create another data frame that contains the location and region of each sensor. This is also used in link_obs_exp to merge to the observed counts and that data frame can be found at the end of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72d71d-21ff-41cb-a0fc-6c14dee591c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all vehicles that are smaller than 5.6 (not all sensors can make that distinction - these are ignored)\n",
    "# this did not work the first time, but the second time it did. weird\n",
    "path = r'/data2/NDW/flow/input'\n",
    "times = (\"/*05.csv.gz\", \"/*06.csv.gz\", \"/*07.csv.gz\", \"/*08.csv.gz\") # the tuple of file types\n",
    "all_files = []\n",
    "for files in times:\n",
    "    all_files.extend(glob.glob(path + files))\n",
    "\n",
    "result_list=[]\n",
    "\n",
    "for filename in all_files:\n",
    "    with open(filename, 'rb') as file:  # this will ensure that only one file is opened at the same time\n",
    "               \n",
    "                #read file\n",
    "                gzip_file = gzip.GzipFile(fileobj=file) #to decompress the gzip file\n",
    "                dat = pd.read_csv(gzip_file)\n",
    "                dat = dat[dat[\"index\"].isin([\"1C\",\"2C\",\"1B\",\"13C\",\"14C\",\"9B\",\"25C\",\"26C\",\"17B\",\"37C\",\"38C\",\"25B\",\"49C\",\"50C\",\"33B\",\"201C\",\"202C\",\"201B\",\"301C\",\"302C\",\"301B\",\"501C\",\"502C\",\"501B\",\"601C\",\"602C\",\"601B\",\"701C\",\"702C\",\"702B\",\"801C\",\"802C\",\"801B\",\"401C\",\"402C\",\"401B\",\"61C\",\"62C\",\"41B\",\"73C\",\"74C\",\"49B\",\"85C\",\"86C\",\"57B\",\"97C\",\"98C\",\"65B\",\"109C\",\"110C\",\"73B\"])]\n",
    "                \n",
    "                #preprocess traffic counts\n",
    "                dat = dat.loc[dat['value'] % 60 == 0] # to remove implausible values in \"value\", since they should be the observed count times 60\n",
    "                dat[\"value\"] = dat[\"value\"] / 60 #divide each value observation by 60 to get the actual count per minute\n",
    "                dat[\"values_used\"] = dat[\"values_used\"].replace(np.nan, np.inf) #for the aggregation, python treats NaNs as 0's. To circumvent that, I change them to np.inf\n",
    "                dat = dat.groupby([\"location_id\", \"time\"], as_index = False).sum()\n",
    "                dat[\"values_used\"] = dat[\"values_used\"].replace(np.inf, np.nan) #change np.inf back to NaN\n",
    "                \n",
    "                #preprocess time column\n",
    "                dat[\"time\"] = dat[\"time\"].str.replace(\"T\", \" \")\n",
    "                dat[\"time\"] = dat[\"time\"].str.replace(\"Z\", \" \")\n",
    "                dat['time'] = pd.to_datetime(dat['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "                dat[\"date\"] = dat[\"time\"].dt.date #extract date from datetime\n",
    "                dat[\"month\"] = dat[\"time\"].dt.month #extract month from datetime\n",
    "                \n",
    "                result_list.append(dat)\n",
    "\n",
    "entire_rushhour = pd.concat(result_list, axis=0, ignore_index=True)\n",
    "entire_rushhour.to_csv('entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec19f7-8f56-458f-ab6b-10200bc07e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entire_rushhour = pd.read_csv('entire_rushhour_shortssummed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2e5a7-6680-453a-972f-17500ac874b6",
   "metadata": {},
   "source": [
    "## Format the time column\n",
    "Because of the size of the data frame, this does not work straightforwardly. I first split the data frame into three smaller frames, and then performed multiprocessing on each of these data frames. The CBS server has 24 processing cores, but this still took quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d4257-7efb-4a8c-b130-260a21c2c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entire_rushhour.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe67d1-f907-432d-b6e4-717c1fab6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "(524379951/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444a796-c6a9-40e1-898c-5f8f07d42e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = entire_rushhour.iloc[:174793317]\n",
    "df2 = entire_rushhour.iloc[174793318:349586634]\n",
    "df3 = entire_rushhour.iloc[349586635:524379951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bcbbf-5301-4b1d-a12b-268533ac074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('df1entire_rushhour_shortssummed.csv', index=False) \n",
    "df2.to_csv('df2entire_rushhour_shortssummed.csv', index=False) \n",
    "df3.to_csv('df3entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930273d-7057-4187-aa88-b8bb0efb9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def replace_letters(dat):\n",
    "    #output_dat = dat.copy()\n",
    "    \n",
    "    dat[\"time\"] = dat[\"time\"].apply(lambda text: text.replace(\"T\", \" \"))\n",
    "    dat[\"time\"] = dat[\"time\"].apply(lambda text: text.replace(\"Z\", \" \"))\n",
    "    \n",
    "    return dat\n",
    "    \n",
    "NUM_CORES = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b4b21-43ec-43b7-8466-8e160a90fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_chunks = np.array_split(df1, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "    \n",
    "    processed_df = pd.concat(pool.map(replace_letters, dat_chunks), ignore_index = True)\n",
    "\n",
    "processed_df.to_csv('df1entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf5164-9439-4f5e-89e7-00f0cc45019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['time'] = pd.to_datetime(processed_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "processed_df[\"date\"] = processed_df[\"time\"].dt.date #extract date from datetime\n",
    "processed_df[\"month\"] = processed_df[\"time\"].dt.month #extract month from datetime\n",
    "processed_df.to_csv('df1entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c66635-5780-45f2-8965-e44590fb6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('df2entire_rushhour_shortssummed.csv') \n",
    "dat_chunks = np.array_split(df2, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "    \n",
    "    processed_df = pd.concat(pool.map(replace_letters, dat_chunks), ignore_index = True)\n",
    "\n",
    "processed_df['time'] = pd.to_datetime(processed_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "processed_df[\"date\"] = processed_df[\"time\"].dt.date #extract date from datetime\n",
    "processed_df[\"month\"] = processed_df[\"time\"].dt.month #extract month from datetime    \n",
    "processed_df.to_csv('df2entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffa804-bd2a-40b8-8853-dc6449cfd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('df3entire_rushhour_shortssummed.csv') \n",
    "dat_chunks = np.array_split(df3, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "    \n",
    "    processed_df = pd.concat(pool.map(replace_letters, dat_chunks), ignore_index = True)\n",
    "\n",
    "processed_df['time'] = pd.to_datetime(processed_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "processed_df[\"date\"] = processed_df[\"time\"].dt.date #extract date from datetime\n",
    "processed_df[\"month\"] = processed_df[\"time\"].dt.month #extract month from datetime\n",
    "processed_df.to_csv('df3entire_rushhour_shortssummed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad893a3d-b3ae-414b-ac0f-2ade69ba3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df1entire_rushhour_shortssummed.csv') \n",
    "df2 = pd.read_csv('df2entire_rushhour_shortssummed.csv')\n",
    "df3 = pd.read_csv('df3entire_rushhour_shortssummed.csv')\n",
    "frames = [df1, df2, df3]\n",
    "entire_rushhour = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d952ad-8be2-4738-b9f9-dcef7a785171",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_rushhour = pd.read_csv('entire_rushhour_shortssummed_datetime.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672c6f4-7e1b-4bce-b06b-d11e571a2e51",
   "metadata": {},
   "source": [
    "### Take random 10% sample of all measurements\n",
    "This will still be millions of rows, so preprocessing is still slow. But it is a workable solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b48d31-93f3-46e5-be76-271823aa4d45",
   "metadata": {},
   "source": [
    "Time is not a datetime in the entire rushhour and there is no hour-column. But file is too large to use the entire file anyway, creating column in 10% sample is doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcc6d8-f950-4be2-84db-a312d0154162",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_rushhour['time'] = pd.to_datetime(entire_rushhour['time'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0243daf-ffc8-4e3c-969b-66b1d7f03d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_rushhour['hourminute'] = entire_rushhour[\"time\"].dt.strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da17aa-fdc3-40b2-bc0b-4e453d1afbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_rushhour.to_csv('entire_rushhour_shortssummed_datetime.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617247a4-3523-4e12-aa45-86bbdee0bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rushhour = entire_rushhour.sample(frac = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c953325-ee63-45b3-bc8f-a12146b7cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rushhour['hourminute'] = sampled_rushhour[\"time\"].dt.strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53932fc2-dbf7-4cb8-950c-c7e37813effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rushhour.to_csv(\"sampled_rushhour.csv\", index = False)\n",
    "sampled_rushhour = pd.read_csv(\"sampled_rushhour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08b465-89b2-4b60-806d-8440f904fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rushhour = pd.read_csv(\"sampled_rushhour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a9deb-f8a9-40d3-b977-b9ff4a1f8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.head(sampled_rushhour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fd409-6e45-4b0a-ad88-92a23251358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = entire_rushhour[\"time\"], y = entire_rushhour[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255507b-2c90-4104-b64e-ea5133b219f0",
   "metadata": {},
   "source": [
    "This plot shows us that there are some outliers in the data, but besides that, it is difficult to get a good look at the intensity over time. We can group by hourminute (hh:mm) to get a better look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083894f-74b9-49d0-b451-a8488977ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rushhour_grouped = sampled_rushhour[[\"location_id\", \"hourminute\", \"value\", \"values_used\"]]\n",
    "sampled_rushhour_grouped = sampled_rushhour.groupby([\"location_id\", \"hourminute\"], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd28f1-fa31-4a45-85fa-3699b36e4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = sampled_rushhour_grouped[\"hourminute\"], y = sampled_rushhour_grouped[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffeb0df-f643-4bdb-a6f8-0dac264c3fb9",
   "metadata": {},
   "source": [
    "Group by hourminute (hh:mm) to look at intensity over the rush hour, and to see the peak of the rush hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aff57-3e32-4003-8b35-5b8376564a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourgrouped = sampled_rushhour[[\"hourminute\", \"value\", \"values_used\"]]\n",
    "hourgrouped = sampled_rushhour.groupby([\"hourminute\"], as_index = False).mean()\n",
    "hourgrouped[\"hourminute\"] = pd.to_datetime(hourgrouped[\"hourminute\"], format='%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aeb3f3-e3c2-41e3-a438-a96784965d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize = (8, 5), dpi = 100)\n",
    "plt.scatter(x = hourgrouped[\"hourminute\"], y = hourgrouped[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb17d44-3cea-4b10-b437-267bc6f0c265",
   "metadata": {},
   "source": [
    "Look at one single sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ab26f-28ac-4a7c-aaaa-a7747544cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amsterdam = entire_rushhour.loc[entire_rushhour[\"location_id\"] == \"GAD02_Amstd_29_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f287a7-0b31-4545-a66d-5b3d347a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amsterdam['hourminute'] = amsterdam[\"time\"].dt.strftime('%H:%M')\n",
    "amsterdam_hourgroup =  amsterdam.groupby([\"hourminute\"], as_index = False).mean()\n",
    "amsterdam_daysum =  amsterdam.groupby([\"date\"], as_index = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d0445-f5ec-4989-8325-49fa04d3b540",
   "metadata": {},
   "source": [
    "Intensity grouped by hourminute for one single sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6621d8-28cc-4914-8d49-12571b55039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "amsterdam_hourgroup[\"hourminute\"] = pd.to_datetime(amsterdam_hourgroup[\"hourminute\"], format='%H:%M')\n",
    "plt.scatter(x = amsterdam_hourgroup[\"hourminute\"], y = amsterdam_hourgroup[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5c458-18ea-4d11-99fd-58bc45f6119e",
   "metadata": {},
   "source": [
    "Daily sum for one single sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f7fa1-2944-475b-9cfa-a8686ae8eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "amsterdam_daysum[\"date\"] = pd.to_datetime(amsterdam_daysum[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "figure(figsize = (10, 4), dpi = 100)\n",
    "plt.scatter(x = amsterdam_daysum[\"date\"], y = amsterdam_daysum[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d0857-6c61-4b5e-ad87-eb6079f6adcf",
   "metadata": {},
   "source": [
    "## Load configuration data, prepare longlat and merge to the sampled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286e043-7b20-4604-bb45-fb520d59e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load configuration data\n",
    "meta = pd.read_csv(\"meta211121.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd508dfb-8e1e-4420-905c-c62c3a4c4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.rename(columns={'dgl_loc':'location_id'}, inplace=True)\n",
    "meta = meta[[\"coordinates\", \"location_id\"]] #drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a53377-e672-4b7c-ab04-d350d872b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def replace_letters(meta):\n",
    "    #output_dat = dat.copy()\n",
    "    \n",
    "    meta[[\"longitude\", \"latitude\"]] = meta[\"coordinates\"].str.split(\",\", expand = True)\n",
    "    meta[\"longitude\"] = meta[\"longitude\"].str[1:]\n",
    "    meta[\"latitude\"] = meta[\"latitude\"].str.rstrip(\"]\")\n",
    "    \n",
    "    return meta\n",
    "    \n",
    "NUM_CORES = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b671c86-267a-4fbe-8fb8-00497b2741b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_chunks = np.array_split(meta, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "    \n",
    "    meta = pd.concat(pool.map(replace_letters, dat_chunks), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66b630-c863-46e6-975a-7ccc9431c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = sampled_rushhour.merge(meta, on='location_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0dc30-a365-45a2-b079-d8405ca22a0f",
   "metadata": {},
   "source": [
    "## Plot daily intensity on map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad47cb-a52b-4bf1-8a64-b8346cbea93b",
   "metadata": {},
   "source": [
    "Group by date to get the daily mean traffic intensity per sensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ec21a-2d35-47c8-846d-d1cce9b84c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_grouped = dat[[\"location_id\", \"longitude\", \"latitude\", \"coordinates\", \"date\", \"value\", \"values_used\"]]\n",
    "dat_grouped =  dat_grouped.groupby([\"location_id\", \"longitude\", \"latitude\", \"coordinates\"], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a041d-68ac-4a6e-937d-f00bbb945612",
   "metadata": {},
   "source": [
    "Import geopandas and NL map, create geopandas DF and map sensor counts onto nl map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d82847-d597-441a-82d5-b8cfd55b021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d7a82-eebf-45b3-8e75-b8ab9253de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_year = gpd.GeoDataFrame(\n",
    "    dat_grouped, geometry=gpd.points_from_xy(dat_grouped.longitude, dat_grouped.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30375ce8-62f7-4e63-bfc1-50cc292a470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlmap = gpd.read_file(\"map/wk_2019.shp\")\n",
    "nlmap = nlmap.to_crs(\"EPSG:4326\") #change CRS to fit the longitude/latitude of sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d56dd7-2e1c-4750-b0e1-3ed700230695",
   "metadata": {},
   "source": [
    "Use \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d96fb8-9aea-4863-8987-bebbf4099233",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (15,15)) \n",
    "nlmap.plot(ax = ax, alpha = 1, color = \"grey\")\n",
    "gdf_year.plot(ax = ax, markersize =0.05*gdf_year[\"value\"]**2, color = \"blue\", marker = \"o\") \n",
    "plt.title('10% Sample of active traffic sensors in 2019\\nSize of point depends on average traffic intensity in 2019', fontsize = 20)\n",
    "#gdf[gdf[\"value\"] == \"bluetooth\"].plot(ax = ax, markersize = 20, color = \"red\", marker = \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee22c90-054c-4f60-bc7e-6ed33d267e79",
   "metadata": {},
   "source": [
    "#### Get maps for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91551ec1-b200-4b9c-9bcf-be3fe0350fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {1: \"Winter\",\n",
    "       2: \"Winter\",\n",
    "       3: \"Spring\",\n",
    "       4: \"Spring\",\n",
    "       5: \"Spring\",\n",
    "       6: \"Summer\",\n",
    "       7: \"Summer\",\n",
    "       8: \"Summer\",\n",
    "       9: \"Fall\",\n",
    "       10: \"Fall\",\n",
    "       11: \"Fall\",\n",
    "       12: \"Winter\"}\n",
    "dat['season'] = dat['month'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dadbd-f59c-4621-b15e-11aad3cc9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_grouped = dat[[\"location_id\", \"longitude\", \"latitude\", \"coordinates\",  \"value\", \"values_used\", \"month\"]]\n",
    "dat_grouped =  dat_grouped.groupby([\"location_id\", \"longitude\", \"latitude\", \"coordinates\", \"month\"], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364a6d1-9626-46b5-83d1-b90ee4cc89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value for better representation on map \n",
    "dat_grouped[\"value2\"] = dat_grouped[\"value\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e092d-96c4-4cdc-a8b7-289825cbc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_month = gpd.GeoDataFrame(\n",
    "    dat_grouped, geometry=gpd.points_from_xy(dat_grouped.longitude, dat_grouped.latitude))\n",
    "\n",
    "nlmap = gpd.read_file(\"map/wk_2019.shp\")\n",
    "nlmap = nlmap.to_crs(\"EPSG:4326\") #change CRS to fit the longitude/latitude of sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7a6dd-7b1d-4cce-8dac-5e7e97ff201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# August plot\n",
    "gdf_aug = gdf_month.loc[gdf_month[\"month\"] == 8]\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (15,15)) \n",
    "nlmap.plot(ax = ax, alpha = 1, color = \"grey\")\n",
    "gdf_aug.plot(ax = ax, markersize =0.05*gdf_aug[\"value2\"], color = \"blue\", marker = \"o\") \n",
    "plt.title('10% Sample of active traffic sensors in 08/2019\\nSize of point depends on average traffic intensity in 08/2019', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545970e-f968-4db2-8a24-e8ba157f8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May plot\n",
    "gdf_may = gdf_month.loc[gdf_month[\"month\"] == 5]\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (15,15)) \n",
    "nlmap.plot(ax = ax, alpha = 1, color = \"grey\")\n",
    "gdf_may.plot(ax = ax, markersize =0.05*gdf_may[\"value2\"], color = \"blue\", marker = \"o\") \n",
    "plt.title('10% Sample of active traffic sensors in 05/2019\\nSize of point depends on average traffic intensity in 05/2019', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fa068-7f97-47c3-9aed-d7597d827e5c",
   "metadata": {},
   "source": [
    "### Splitting the sensors into regions\n",
    "We use one of CBS' shapefiles for provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa0e2f-648a-471a-8a15-cdc8757170d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlmap_regions = gpd.read_file(\"Data/CBS/Shapefiles/Province/pv_2019.shp\")\n",
    "nlmap_regions = nlmap_regions.to_crs(\"EPSG:4326\") #change CRS to fit the longitude/latitude of sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cc843-3b50-4af3-a474-7f1bae68f6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,10)) \n",
    "nlmap_regions.plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcb9bb-8757-4a6f-a499-fabb542cc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://opendata.cbs.nl/statline/#/CBS/en/dataset/37259eng/table?ts=1639757076435\n",
    "densdic = {\"Groningen\": 251,\n",
    "       \"Friesland\": 194,\n",
    "       \"Drenthe\": 187,\n",
    "       \"Overijssel\": 348,\n",
    "       \"Flevoland\": 295,\n",
    "       \"Gelderland\": 417,\n",
    "       \"Utrecht\": 904,\n",
    "       \"Noord-Holland\": 1071,\n",
    "       \"Zuid-Holland\": 1361,\n",
    "       \"Zeeland\": 215,\n",
    "       \"Noord-Brabant\": 519,\n",
    "       \"Limburg\": 520}\n",
    "\n",
    "nlmap_regions[\"pop_density\"] = nlmap_regions[\"PV_NAAM\"].map(densdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3e93c-08c4-4046-8a19-7e9259b60fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlmap_regions[\"pop_density_log\"] = np.log(nlmap_regions[\"pop_density\"])\n",
    "nlmap_regions[\"pop_density_lognorm\"] = (nlmap_regions[\"pop_density_log\"] - np.log(nlmap_regions[\"pop_density\"].min()) ) / ( np.log(nlmap_regions[\"pop_density\"].max()) -  np.log(nlmap_regions[\"pop_density\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06435bb2-9784-4017-a47b-f7a8e5b4b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,10)) \n",
    "nlmap_regions.plot(ax = ax, column = \"pop_density_log\", cmap = \"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8cb4d-272a-47bf-ba8b-d13db5b32055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.path as mpath\n",
    "circle = mpath.Path.unit_circle()\n",
    "verts = np.copy(circle.vertices)\n",
    "verts[:, 0] *= 1.618\n",
    "ellipt_marker = mpath.Path(verts, circle.codes)\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (15,10)) \n",
    "nlmap_regions.plot(ax = ax, column = \"pop_density_log\", cmap = \"Blues\")\n",
    "gdf_year.plot(ax = ax, markersize =0.04*gdf_year[\"value\"]**2, color = \"orangered\", marker = ellipt_marker) \n",
    "plt.title('10% Sample of active traffic sensors in 2019\\nSize of point depends on average traffic intensity in 2019\\ndarker regional shade implies denser population', fontsize = 20)\n",
    "#gdf[gdf[\"value\"] == \"bluetooth\"].plot(ax = ax, markersize = 20, color = \"red\", marker = \"o\")\n",
    "\n",
    "plt.savefig('map with density.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b7419-27ca-48a0-a9f4-a50592ec6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making elipse marker for better visual in word document\n",
    "import matplotlib.path as mpath\n",
    "circle = mpath.Path.unit_circle()\n",
    "verts = np.copy(circle.vertices)\n",
    "verts[:, 0] *= 1.618\n",
    "ellipt_marker = mpath.Path(verts, circle.codes)\n",
    "\n",
    "#plot\n",
    "fig,ax = plt.subplots(figsize = (15,10)) \n",
    "nlmap_regions.plot(ax = ax, column = \"pop_density_log\", cmap = \"Blues\")\n",
    "gdf_year.plot(ax = ax, markersize =0.04*gdf_year[\"value\"]**2, color = \"orangered\", marker = ellipt_marker) \n",
    "#plt.title('10% Sample of active traffic sensors in 2019\\nSize of point depends on average traffic intensity in 2019\\ndarker regional shade implies denser population', fontsize = 20)\n",
    "ax.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
    "plt.savefig('map with density.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459376a6-c9ec-4dae-97ff-d07975b691c0",
   "metadata": {},
   "source": [
    "# Create CSV for further steps in link_obs_exp.ipynb\n",
    "This dataframe contains the location of the sensors, which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf0095-c67a-49c3-bc19-3ba44b1d8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_year.crs  = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8f08b-e52d-44d7-ae03-965699851f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlmap_classified = gpd.sjoin(gdf_year, nlmap_regions, op = 'within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1070be-5e29-42a2-aac7-68e2a7cae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlmap_classified_df = pd.DataFrame(nlmap_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8be15-5ee3-46d1-b601-8b0f0c5d4c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlmap_classified_df.to_csv('nlmap_classified_df.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
